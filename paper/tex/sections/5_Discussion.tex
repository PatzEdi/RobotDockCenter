\section{Discussion}
% Discussion of your findings and their implications
There have been many instances of robotic entities docking on their own without human intervention, even without the use of vision-based systems. For example, infrared sensors have been used for a while in order to detect and align with the dock \citep{quilez:hal-01147332}. Other sensors include LiDAR and ultrasonic sensors. Our method of using only vision would work much better with these systems, however, when paired in conjunction to a pathfinding algorithm, likely based on machine learning. We would use the same cameras for docking to automate path making and traveling around an area. This allows for less hardware and complications regarding the overall robot functionality. Plus, relying on vision-based systems is much more flexible and adaptable to future and different environments.
% Discussion of the limitations of your approach, and possibly how we can improve them.
\subsection{Limitations}
There are a two main limitations to our approach that must be discussed. The first one is that we don't know if the robot is centered upon initializaiton. For example, if we were to play the robot directly in front of the dock (target 2) without it having reached target 1 first, we don't know if we are centered. Therefore, we search for target 1 first, even when it is not necessary. This limitation largely arises due to our model not outputting a predicted distance from what is considered centered with the dock. This is because it would be extremely time-consuming to label images with the distance between the center of the dock and the robot. This could be fixed by implementing a manual switch that forces target 2 locking and ignores the first procedure of having to lock and arrive at target 1.

Our second limitation is that our model doesn't know what to do when there is an obstacle in front of it during the docking process. For example, if the docking procedure starts, and then there is a box placed in front of it during the procedure, the robot would not go around the box, but would rather either stop or search for the target it was locked on to. In short, it can't go around objects during the docking process.

\subsection{Future Work}
% Then, we can also discuss the implementation of the YOLO model and how that would help simplify things. Also point out that we did not do this here as we needed to determine the minimum amount of model complexity for the docking to be achieved.
In the future, we can implement a single YOLO model to detect both target 1 and the dock (target 2). This would allow us to simplify our approach by not having multiple models, but rather just one. Not only that, but we could also detect if a target is in the image. This means that, for example, if we start the docking process and the YOLO model does not detect target 1, we can rotate the robot until target 1 is detected. Once target 1 is detected, out docking procedure starts.
